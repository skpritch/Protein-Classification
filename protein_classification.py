# -*- coding: utf-8 -*-
"""Protein_Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sErWMHIRjsGX4DGyQAQzbxfeOHwZIfO4
"""

!pip install biopython tensorflow scikit-learn

# Import necessary libraries

from google.colab import drive
drive.mount('/content/drive')

import re
import numpy as np
import pandas as pd
from collections import Counter
import itertools
from Bio.SeqUtils.ProtParam import ProteinAnalysis
from sklearn.preprocessing import StandardScaler, LabelEncoder
import matplotlib.pyplot as plt
import seaborn as sns

import tensorflow as tf
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, LSTM, Masking
from tensorflow.keras.optimizers import Adam

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

from scipy.stats import randint, loguniform, uniform
import random
from sklearn.model_selection import RandomizedSearchCV
from torch.utils.data import TensorDataset, DataLoader
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression

from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,
    confusion_matrix, classification_report
)

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/Stanford/Junior_Year/ML_Tutorial/Protein_Classification
!ls

# Read in data
data = pd.read_csv('balanced_seq.csv')

# Replace unknown amino acids with Alanine
def replace_unknown_amino_acids(sequence, replacement='A'):
    return sequence.replace('X', replacement)

# Apply the function to your DataFrame
data['CleanSequence'] = data['Sequence'].apply(replace_unknown_amino_acids)
print(data.head())

df = data[['Entry', 'ProteinClass']]
df_all = data[['Entry', 'ProteinClass']]
df_NN = data[['Entry', 'ProteinClass']]
df_RNN = data[['Entry', 'Sequence', 'ProteinClass']]

# Add Sequence Length
# Accuracies: 0.736, 0.236, 0.766

df.loc[:, 'SequenceLength'] = data['CleanSequence'].apply(len)
df_seq = pd.concat([df, data['CleanSequence'].apply(len)], axis=1)
df_NN.loc[:, 'SequenceLength'] = data['CleanSequence'].apply(len)
df_all.loc[:, 'SequenceLength'] = data['CleanSequence'].apply(len)
print(df.head())

# Amino Acid Frequencies
# Accuracies: 0.903, 0.743, 0.88

# List of standard amino acids
amino_acids = list('ACDEFGHIKLMNPQRSTVWY')

def calculate_aac(sequence):
    analysed_seq = ProteinAnalysis(sequence)
    percent = analysed_seq.get_amino_acids_percent()
    return [percent.get(aa, 0) for aa in amino_acids]

# Calculate AAC for each sequence
df_aac = data['CleanSequence'].apply(calculate_aac)
aac_df = pd.DataFrame(df_aac.tolist(), columns=amino_acids)

# Concatenate AAC features to the main DataFrame
df = pd.concat([df, aac_df], axis=1)
df_freq = pd.concat([df, aac_df], axis=1)
df_all = pd.concat([df_all, aac_df], axis=1)
df_NN = pd.concat([df_NN, aac_df], axis=1)
print(df.head())

# All possible dipeptide frequencies
# Accuracies: 0.916, 0.93, 0.926

amino_acids = list('ACDEFGHIKLMNPQRSTVWY')
dipeptides = [''.join(dp) for dp in itertools.product(amino_acids, repeat=2)]

def calculate_dipeptide_comp(sequence):
    dipeptide_counts = Counter([sequence[i:i+2] for i in range(len(sequence)-1)])
    total = sum(dipeptide_counts.values())
    return [dipeptide_counts.get(dp, 0) / total for dp in dipeptides]

# Calculate dipeptide composition
df_dipep = data['CleanSequence'].apply(calculate_dipeptide_comp)
dipeptide_df = pd.DataFrame(df_dipep.tolist(), columns=[dp for dp in dipeptides])

# Concatenate dipeptide features to the main DataFrame
df = pd.concat([df, dipeptide_df], axis=1)
df_dipep = pd.concat([df, dipeptide_df], axis=1)
df_all = pd.concat([df_all, dipeptide_df], axis=1)
df_NN = pd.concat([df_NN, dipeptide_df], axis=1)
print(df.head())

# Reduced Amino Acid Alphabet

# Define amino acid groupings
groupings = {
    'Aliphatic': 'GAVLIPM',
    'Aromatic': 'FYW',
    'Polar_Uncharged': 'STCQN',
    'Positive_Charged': 'HKR',
    'Negative_Charged': 'DE',
}

def reduce_alphabet(sequence):
    reduced_sequence = ''
    for aa in sequence:
        for group, members in groupings.items():
            if aa in members:
                reduced_sequence += members[0]  # Use the first amino acid of the group
                break
        else:
            reduced_sequence += 'X'  # For amino acids not in any group
    return reduced_sequence

# Apply reduction
data['RedSequence'] = data['CleanSequence'].apply(reduce_alphabet)

# Calculate frequencies of each group
group_representatives = [members[0] for members in groupings.values()]

# Reduced Amino Acid Alphabet Frequencies
# Accuracies: 0.856, 0.423, 0.856

def calculate_group_freq(sequence):
    count = Counter(sequence)
    total = sum(count.values())
    return [count.get(gr, 0) / total for gr in group_representatives]

group_freq = data['RedSequence'].apply(calculate_group_freq)
group_freq_df = pd.DataFrame(group_freq.tolist(), columns=group_representatives)

# Concatenate group frequency features to the main DataFrame
df_red_freq = pd.concat([df, group_freq_df], axis=1)
df_all = pd.concat([df_all, group_freq_df], axis=1)
df_NN = pd.concat([df, group_freq_df], axis=1)

print(df_all.head())

# N-Gram Profiles of Reduced Amino Acid Alphabet
# Accuracies 0.893, 0.713, 0.9

def generate_ngrams(group_representatives, n):
    return [''.join(ng) for ng in itertools.product(group_representatives, repeat=n)]

# Function to calculate n-gram frequencies
def calculate_ngram_freq(sequence, n, ngrams):
    ngram_counts = Counter([sequence[i:i+n] for i in range(len(sequence) - n + 1)])
    total = sum(ngram_counts.values())
    return [ngram_counts.get(ng, 0) / total for ng in ngrams]

# Compute and append n-gram frequencies for 2, 3, and 4 grams
for n in [2,3]:
    ngrams = generate_ngrams(group_representatives, n)
    gram_df = data['RedSequence'].apply(lambda x: calculate_ngram_freq(x, n, ngrams))
    gram_df = pd.DataFrame(gram_df.tolist(), columns=[f"{ng}" for ng in ngrams])
    df_red_ngram = pd.concat([df, gram_df], axis=1)
    df_all = pd.concat([df_all, gram_df], axis=1)
    df = pd.concat([df, gram_df], axis=1)
    df_NN = pd.concat([df_NN, gram_df], axis=1)

# Print the resulting DataFrame
print(df_all.head())

# Add Protein Properties Using BioPython
# Accuracies: 0.896, 0.56, 0.886

def calculate_physicochemical_props(sequence):
    analysis = ProteinAnalysis(sequence)

    flexibility = analysis.flexibility()
    avg_flexibility = sum(flexibility) / len(flexibility)

    helix, sheet, turn = analysis.secondary_structure_fraction()

    return {
        'MolecularWeight': analysis.molecular_weight(),
        'IsoelectricPoint': analysis.isoelectric_point(),
        'Aromaticity': analysis.aromaticity(),
        'InstabilityIndex': analysis.instability_index(),
        'Flexibility': avg_flexibility,
        'Helix': helix,
        'Sheet': sheet,
        'Turn': turn,
        'Gravy': analysis.gravy(),
    }

# Calculate physicochemical properties
df_props = data['CleanSequence'].apply(calculate_physicochemical_props).apply(pd.Series)

# Concatenate physicochemical features to the main DataFrame
df_prop = pd.concat([df, df_props], axis=1)
df_all = pd.concat([df_all, df_props], axis=1)

print(df_prop.head())

def evaluate_model(model, X_test, y_test, model_name):

    y_pred = model.predict(X_test)

    # Evaluation
    accuracy = accuracy_score(y_test, y_pred)
    print(f"\n=== {model_name} ===")
    print(f"Accuracy after tuning: {accuracy:.4f}")
    print("Classification Report:")
    print(classification_report(y_test, y_pred))

    # Confusion matrix
    classes = model.classes_
    conf_matrix = confusion_matrix(y_test, y_pred, labels=classes)
    plt.figure(figsize=(10,8))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
                xticklabels=classes, yticklabels=classes)
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')
    plt.title(f'Confusion Matrix for {model_name}')
    plt.show()

def run_models(df):
    # Define X and y
    y = df['ProteinClass']
    X = df.drop(['Entry', 'ProteinClass'], axis=1)
    print("Feature sample:")
    print(X.head())
    print("\nTarget sample:")
    print(y.head())

    # Split the data
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42)

    # Feature scaling (important for logistic regression and gradient boosting)
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    ### Logistic Regression ###
    # Define the parameter distribution for 'C'
    param_dist_lr = {
        'C': loguniform(1e-3, 1e3),
        'penalty': ['l2'],
        'solver': ['lbfgs'],
        'multi_class': ['multinomial'],
        'max_iter': [500, 1000, 2000],
        'random_state': [42]
    }

    # Initialize Logistic Regression model
    lr_model = LogisticRegression()

    # Set up RandomizedSearchCV
    random_search_lr = RandomizedSearchCV(
        estimator=lr_model,
        param_distributions=param_dist_lr,
        n_iter=50,
        cv=5,
        scoring='accuracy',
        random_state=42,
        n_jobs=-1
    )

    # Fit the RandomizedSearchCV to the training data
    random_search_lr.fit(X_train_scaled, y_train)

    # Get the best estimator
    best_lr_model = random_search_lr.best_estimator_
    print("Best parameters found for Logistic Regression:")
    print(random_search_lr.best_params_)

    # Evaluate Logistic Regression
    evaluate_model(best_lr_model, X_test_scaled, y_test, model_name="Logistic Regression")

    ### Random Forest Classifier ###
    # Define the parameter distribution
    param_dist_rf = {
        'n_estimators': randint(10, 1000),
        'max_depth': [None],
        'min_samples_split': randint(2, 5),
        'min_samples_leaf': randint(2, 5),
        'bootstrap': [True, False],
        'random_state': [42]
    }

    # Initialize Random Forest model
    rf_model = RandomForestClassifier()

    # Set up RandomizedSearchCV
    random_search_rf = RandomizedSearchCV(
        estimator=rf_model,
        param_distributions=param_dist_rf,
        n_iter=50,
        cv=5,
        scoring='accuracy',
        random_state=42,
        n_jobs=-1
    )

    # Fit the RandomizedSearchCV to the training data
    random_search_rf.fit(X_train, y_train)

    # Get the best estimator
    best_rf_model = random_search_rf.best_estimator_
    print("\nBest parameters found for Random Forest:")
    print(random_search_rf.best_params_)

    # Evaluate Random Forest
    evaluate_model(best_rf_model, X_test, y_test, model_name="Random Forest")

    ### Gradient Boosting Classifier ###
    # Initialize Gradient Boosting with hyperparameters
    gb_model = GradientBoostingClassifier(
        n_estimators=100,
        learning_rate=0.1,
        max_depth=3,
        min_samples_split=2,
        min_samples_leaf=1,
        random_state=42
    )

    # Train the model
    gb_model.fit(X_train_scaled, y_train)

    # Evaluate Gradient Boosting
    evaluate_model(gb_model, X_test_scaled, y_test, model_name="Gradient Boosting")

run_models(df)

run_models(df_all)

datasets = [df_seq, df_freq, df_dipep, df_red_freq, df_red_ngram, df_prop]

for df in datasets:
    run_models(df)

# Generate a new dataframe to train the neural network
# df_NN = data[['Entry', 'ProteinClass']]
print(df_NN.head())

# Split data
y_NN = df_NN['ProteinClass']
X_NN = df_NN.drop(['Entry', 'ProteinClass'], axis=1)
X_train_NN, X_test_NN, y_train_NN, y_test_NN = train_test_split(X_NN.to_numpy(), y_NN.to_numpy(), test_size=0.2, stratify=y_NN, random_state=40)

print(X_train_NN.shape)
print(X_test_NN.shape)

# Create a label encoder
label_encoder = LabelEncoder()

# Fit the encoder on the training labels
label_encoder.fit(y_train_NN)

# Transform both training and testing labels
y_train_encoded = label_encoder.transform(y_train_NN)
y_test_encoded = label_encoder.transform(y_test_NN)

# Convert numpy arrays to PyTorch tensors
X_train_tensor = torch.tensor(X_train_NN, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train_encoded, dtype=torch.long)

X_test_tensor = torch.tensor(X_test_NN, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test_encoded, dtype=torch.long)

# Create datasets and dataloaders
batch_size = 32

train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

test_dataset = TensorDataset(X_test_tensor, y_test_tensor)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)

# Define neural network architecture
class ProteinClassifier(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes):
        super(ProteinClassifier, self).__init__()
        # Hidden layers
        self.fc_layers = nn.ModuleList()  # Use a ModuleList to store layers
        # self.dropout_layers = nn.ModuleList()
        for i in range(hidden_layers):
            self.fc_layers.append(nn.Linear(input_size if i == 0 else hidden_size, hidden_size)) # Input size changes after first layer
            # self.dropout_layers.append(nn.Dropout(p=0.10))
        # Output layer
        self.fc_out = nn.Linear(hidden_size, num_classes)
        # Activation function
        self.relu = nn.ReLU()

    def forward(self, out):
        # Pass through hidden layers using the ModuleList
        for i in range(hidden_layers):
            out = self.fc_layers[i](out)
            # out = self.dropout_layers[i](out)
            out = self.relu(out)
        # Output layer (no activation here because we'll use CrossEntropyLoss)
        out = self.fc_out(out)
        return out  # Return raw logits

input_size = 156
hidden_size = 175
num_classes = 10
hidden_layers = 5

model = ProteinClassifier(input_size, hidden_size, num_classes)

criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.005) # , weight_decay=1e-4

### Running the Neural Network ###

num_epochs = 75
val_best = 0.0
patience = 15
epochs_without_improvement = 0

train_losses = []
test_losses = []
train_accuracies = []
test_accuracies = []

for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    all_labels = []
    all_preds = []

    for inputs, labels in train_loader:
        # Zero the parameter gradients
        optimizer.zero_grad()

        # Forward pass
        outputs = model(inputs)

        # Compute loss
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

        # Collect predictions and labels for accuracy calculation
        _, preds = torch.max(outputs, 1)
        all_labels.extend(labels.cpu().numpy())
        all_preds.extend(preds.cpu().numpy())

    # Compute average loss and accuracy over the epoch
    avg_loss = running_loss / len(train_loader)
    train_accuracy = accuracy_score(all_labels, all_preds)
    train_losses.append(avg_loss)
    train_accuracies.append(train_accuracy)

    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Training Accuracy: {train_accuracy:.4f}')

    # Validation Step
    model.eval()
    val_running_loss = 0.0
    val_all_labels = []
    val_all_preds = []

    with torch.no_grad():
        for inputs, labels in test_loader:
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            val_running_loss += loss.item()

            _, preds = torch.max(outputs, 1)
            val_all_labels.extend(labels.cpu().numpy())
            val_all_preds.extend(preds.cpu().numpy())

    val_loss = val_running_loss / len(test_loader)
    test_accuracy = accuracy_score(val_all_labels, val_all_preds)
    test_losses.append(val_loss)
    test_accuracies.append(test_accuracy)

    print(f'Test Loss: {val_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')

    """# Early stopping
    if test_accuracy > val_best:
        val_best = test_accuracy
        epochs_without_improvement = 0
    else:
        epochs_without_improvement += 1

    if epochs_without_improvement >= patience:
        print(f"Early stopping at epoch {epoch+1}")
        break
"""
# Plot training and validation loss over epochs
plt.figure(figsize=(10,5))
plt.plot(range(1, len(train_losses)+1), train_losses, label='Training Loss')
plt.plot(range(1, len(test_losses)+1), test_losses, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Loss over Epochs')
plt.legend()
plt.show()

# Plot training and validation accuracy over epochs
plt.figure(figsize=(10,5))
plt.plot(range(1, len(train_accuracies)+1), train_accuracies, label='Training Accuracy')
plt.plot(range(1, len(test_accuracies)+1), test_accuracies, label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Training and Validation Accuracy over Epochs')
plt.legend()
plt.show()

# Convert predictions and true labels back to class names
y_test_labels = label_encoder.inverse_transform(val_all_labels)
test_preds_labels = label_encoder.inverse_transform(val_all_preds)

# Generate confusion matrix and plot
conf_matrix = confusion_matrix(y_test_labels, test_preds_labels, labels=label_encoder.classes_)
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix with Class Names')
plt.show()

print(df_RNN.head())

# Using an RNN

# Tokenization
tokenizer = Tokenizer(char_level=True)
tokenizer.fit_on_texts(df_RNN['Sequence'])
sequences = tokenizer.texts_to_sequences(df_RNN['Sequence'])

# Padding the sequences
max_seq_length = max(len(seq) for seq in sequences)
X = pad_sequences(sequences, maxlen=max_seq_length, padding='post')

# Encode protein class labels to compare to output nodes
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(df_RNN['ProteinClass'])
y = tf.keras.utils.to_categorical(y, num_classes=len(label_encoder.classes_))

# Split the data for training vs. testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)

# Build the model
model = Sequential()
model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=64, input_length=max_seq_length))
# model.add(Masking(mask_value=0.0, input_shape=(max_seq_length,)))
model.add(SimpleRNN(32, activation='tanh'))
model.add(Dense(num_classes, activation='softmax'))

# Compile the Model
optimizer = Adam(learning_rate=0.005)
model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

# Train the Model
history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=32)

# Print Model Evaluation Statistics
loss, accuracy = model.evaluate(X_test, y_test)
print(f'Test Loss: {loss:.4f}')
print(f'Test Accuracy: {accuracy:.4f}')

# Predict classes for the test set
y_pred_probs = model.predict(X_test)
y_pred = np.argmax(y_pred_probs, axis=1)

# Accuracy plot
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.legend()
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Training and Validation Accuracy')
plt.show()

# Loss plot
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.legend()
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.show()

# Convert y_test to multiclass format
y_test_multiclass = np.argmax(y_test, axis=1)

# Generate confusion matrix
conf_matrix = confusion_matrix(y_test_multiclass, y_pred)

# Get class labels
class_names = label_encoder.classes_

# Plot confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=class_names, yticklabels=class_names)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix of Protein Class Prediction')
plt.show()

# Print classification report
print('Classification Report')
print(classification_report(y_test_multiclass, y_pred, target_names=class_names))

# Using an LSTM

# Tokenization
tokenizer = Tokenizer(char_level=True)
tokenizer.fit_on_texts(df_RNN['Sequence'])
sequences = tokenizer.texts_to_sequences(df_RNN['Sequence'])

# Padding the sequences
max_seq_length = max(len(seq) for seq in sequences)
X = pad_sequences(sequences, maxlen=max_seq_length, padding='post')

# Encode protein class labels to compare to output nodes
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(df_RNN['ProteinClass'])
y = tf.keras.utils.to_categorical(y, num_classes=len(label_encoder.classes_))

# Split the data for training vs. testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# Build the model
model = Sequential()
model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=64, input_length=max_seq_length))
# model.add(Masking(mask_value=0.0, input_shape=(max_seq_length,)))
model.add(LSTM(units=64, activation='relu', recurrent_activation='hard_sigmoid'))
model.add(Dense(num_classes, activation='softmax'))

# Compile the Model
optimizer = Adam(learning_rate=0.005)
model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

# Train the Model
history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=32)

# Print Model Evaluation Statistics
loss, accuracy = model.evaluate(X_test, y_test)
print(f'Test Loss: {loss:.4f}')
print(f'Test Accuracy: {accuracy:.4f}')

# Predict classes for the test set
y_pred_probs = model.predict(X_test)
y_pred = np.argmax(y_pred_probs, axis=1)

# Accuracy plot
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.legend()
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Training and Validation Accuracy')
plt.show()

# Loss plot
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.legend()
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.show()

# Convert y_test to multiclass format
y_test_multiclass = np.argmax(y_test, axis=1)

# Generate confusion matrix
conf_matrix = confusion_matrix(y_test_multiclass, y_pred)

# Get class labels
class_names = label_encoder.classes_

# Plot confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=class_names, yticklabels=class_names)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix of Protein Class Prediction')
plt.show()

# Print classification report
print('Classification Report')
print(classification_report(y_test_multiclass, y_pred, target_names=class_names))